apiVersion: v1
data:
  filebeat.yml: "######################## Filebeat Configuration ############################\n\n#
    This file is a full configuration example documenting all non-deprecated\n# options
    in comments. For a shorter configuration example, that contains only\n# the most
    common options, please see filebeat.yml in the same directory.\n#\n# You can find
    the full configuration reference here:\n# https://www.elastic.co/guide/en/beats/filebeat/index.html\n\n\n#==========================
    \ Modules configuration ============================\nfilebeat.modules:   \n\n\n#===========================
    Filebeat prospectors =============================\n\n# List of prospectors to
    fetch data.\nfilebeat.prospectors:\n# Each - is a prospector. Most options can
    be set at the prospector level, so\n# you can use different prospectors for various
    configurations.\n# Below are the prospector specific configurations.\n\n# Type
    of the files. Based on this the way the file is read is decided.\n# The different
    types cannot be mixed in one prospector\n\n#------------------------------ Log
    prospector --------------------------------\n- type: log\n  paths:\n    - /bro/logger/*.log\n
    \ tags: [\"bro\"]\n\n  ### JSON configuration\n\n  # Decode JSON options. Enable
    this if your logs are structured in JSON.\n  # JSON key on which to apply the
    line filtering and multiline settings. This key\n  # must be top level and its
    value must be string, otherwise it is ignored. If\n  # no text key is defined,
    the line filtering and multiline features cannot be used.\n  #json.message_key:\n\n
    \ # By default, the decoded JSON is placed under a \"json\" key in the output
    document.\n  # If you enable this setting, the keys are copied top level in the
    output document.\n  #json.keys_under_root: false\n\n  # If keys_under_root and
    this setting are enabled, then the values from the decoded\n  # JSON object overwrite
    the fields that Filebeat normally adds (type, source, offset, etc.)\n  # in case
    of conflicts.\n  #json.overwrite_keys: false\n\n  # If this setting is enabled,
    Filebeat adds a \"error.message\" and \"error.key: json\" key in case of JSON\n
    \ # unmarshaling errors or when a text key is defined in the configuration but
    cannot\n  # be used.\n  #json.add_error_key: true\n\n  ### Multiline options\n\n
    \ # Mutiline can be used for log messages spanning multiple lines. This is common\n
    \ # for Java Stack Traces or C-Line Continuation\n\n  # The regexp Pattern that
    has to be matched. The example pattern matches all lines starting with [\n  #multiline.pattern:
    ^\\[\n\n  # Defines if the pattern set under pattern should be negated or not.
    Default is false.\n  #multiline.negate: false\n\n  # Match can be set to \"after\"
    or \"before\". It is used to define if lines should be append to a pattern\n  #
    that was (not) matched before or after or as long as a pattern is not matched
    based on negate.\n  # Note: After is the equivalent to previous and before is
    the equivalent to to next in Logstash\n  #multiline.match: after\n\n  # The maximum
    number of lines that are combined to one event.\n  # In case there are more the
    max_lines the additional lines are discarded.\n  # Default is 500\n  #multiline.max_lines:
    500\n\n  # After the defined timeout, an multiline event is sent even if no new
    pattern was found to start a new event\n  # Default is 5s.\n  #multiline.timeout:
    5s\n\n  # Setting tail_files to true means filebeat starts reading new files at
    the end\n  # instead of the beginning. If this is used in combination with log
    rotation\n  # this can mean that the first entries of a new file are skipped.\n
    \ #tail_files: false\n\n  # The Ingest Node pipeline ID associated with this prospector.
    If this is set, it\n  # overwrites the pipeline option from the Elasticsearch
    output.\n  #pipeline:\n\n  # If symlinks is enabled, symlinks are opened and harvested.
    The harvester is openening the\n  # original for harvesting but will report the
    symlink name as source.\n  #symlinks: false\n\n  # Backoff values define how aggressively
    filebeat crawls new files for updates\n  # The default values can be used in most
    cases. Backoff defines how long it is waited\n  # to check a file again after
    EOF is reached. Default is 1s which means the file\n  # is checked every second
    if new lines were added. This leads to a near real time crawling.\n  # Every time
    a new line appears, backoff is reset to the initial value.\n  #backoff: 1s\n\n
    \ # Max backoff defines what the maximum backoff time is. After having backed
    off multiple times\n  # from checking the files, the waiting time will never exceed
    max_backoff independent of the\n  # backoff factor. Having it set to 10s means
    in the worst case a new line can be added to a log\n  # file after having backed
    off multiple times, it takes a maximum of 10s to read the new line\n  #max_backoff:
    10s\n\n  # The backoff factor defines how fast the algorithm backs off. The bigger
    the backoff factor,\n  # the faster the max_backoff value is reached. If this
    value is set to 1, no backoff will happen.\n  # The backoff value will be multiplied
    each time with the backoff_factor until max_backoff is reached\n  #backoff_factor:
    2\n\n  # Max number of harvesters that are started in parallel.\n  # Default is
    0 which means unlimited\n  #harvester_limit: 0\n\n  ### Harvester closing options\n\n
    \ # Close inactive closes the file handler after the predefined period.\n  # The
    period starts when the last line of the file was, not the file ModTime.\n  # Time
    strings like 2h (2 hours), 5m (5 minutes) can be used.\n  #close_inactive: 5m\n\n
    \ # Close renamed closes a file handler when the file is renamed or rotated.\n
    \ # Note: Potential data loss. Make sure to read and understand the docs for this
    option.\n  #close_renamed: false\n\n  # When enabling this option, a file handler
    is closed immediately in case a file can't be found\n  # any more. In case the
    file shows up again later, harvesting will continue at the last known position\n
    \ # after scan_frequency.\n  #close_removed: true\n\n  # Closes the file handler
    as soon as the harvesters reaches the end of the file.\n  # By default this option
    is disabled.\n  # Note: Potential data loss. Make sure to read and understand
    the docs for this option.\n  #close_eof: false\n\n  ### State options\n\n  # Files
    for the modification data is older then clean_inactive the state from the registry
    is removed\n  # By default this is disabled.\n  #clean_inactive: \n  # Removes
    the state for file which cannot be found on disk anymore immediately\n  #clean_removed:
    true\n\n  # Close timeout closes the harvester after the predefined time.\n  #
    This is independent if the harvester did finish reading the file or not.\n  #
    By default this option is disabled.\n  # Note: Potential data loss. Make sure
    to read and understand the docs for this option.\n  #close_timeout: 0\n\n  # Defines
    if prospectors is enabled\n  #enabled: true\n\n#-----------------------------
    Stdin prospector -------------------------------\n# Configuration to use stdin
    input\n#- type: stdin\n\n#------------------------- Redis slowlog prospector ---------------------------\n#
    Experimental: Config options for the redis slow log prospector\n#- type: redis\n
    #hosts: [\"localhost:6379\"]\n  #username:\n  #password:\n  #enabled: true\n  #scan_frequency:
    10s\n\n  # Timeout after which time the prospector should return an error\n  #timeout:
    1s\n\n  # Network type to be used for redis connection. Default: tcp\n  #network:
    tcp\n\n  # Max number of concurrent connections. Default: 10\n  #maxconn: 10\n\n
    \ # Redis AUTH password. Empty by default.\n  #password: foobared\n\n#=========================
    Filebeat global options ============================\n\n# Event count spool threshold
    - forces network flush if exceeded\n#filebeat.spool_size: 2048\n\n# Enable async
    publisher pipeline in filebeat (Experimental!)\n#filebeat.publish_async: false\n\n#
    Defines how often the spooler is flushed. After idle_timeout the spooler is\n#
    Flush even though spool_size is not reached.\n#filebeat.idle_timeout: 5s\n\n#
    Name of the registry file. If a relative path is used, it is considered relative
    to the\n# data path.\n#filebeat.registry_file: ${path.data}/registry\n\n#\n# These
    config files must have the full filebeat config part inside, but only\n# the prospector
    part is processed. All global options like spool_size are ignored.\n# The config_dir
    MUST point to a different directory then where the main filebeat config file is
    in.\n#filebeat.config_dir:\n\n# How long filebeat waits on shutdown for the publisher
    to finish.\n# Default is 0, not waiting.\n#filebeat.shutdown_timeout: 0\n\n# Enable
    filebeat config reloading\n#filebeat.config.prospectors:\n  #enabled: false\n
    \ #path: configs/*.yml\n  #reload.enabled: true\n  #reload.period: 10s\n\n#================================
    General ======================================\n\n# The name of the shipper that
    publishes the network data. It can be used to group\n# all the transactions sent
    by a single shipper in the web interface.\n# If this options is not defined, the
    hostname is used.\n#name:\n\n# The tags of the shipper are included in their own
    field with each\n# transaction published. Tags make it easy to group servers by
    different\n# logical properties.\n#tags: [\"service-X\", \"web-tier\"]\n\n# Optional
    fields that you can specify to add additional information to the\n# output. Fields
    can be scalar values, arrays, dictionaries, or any nested\n# combination of these.\n#fields:\n#
    \ env: staging\n\n# If this option is set to true, the custom fields are stored
    as top-level\n# fields in the output document instead of being grouped under a
    fields\n# sub-dictionary. Default is false.\n#fields_under_root: false\n\n# Internal
    queue size for single events in processing pipeline\n#queue_size: 1000\n\n# The
    internal queue size for bulk events in the processing pipeline.\n# Do not modify
    this value.\n#bulk_queue_size: 0\n\n# Sets the maximum number of CPUs that can
    be executing simultaneously. The\n# default is the number of logical CPUs available
    in the system.\n#max_procs:\n\n#================================ Processors ===================================\n\n#
    Processors are used to reduce the number of fields in the exported event or to\n#
    enhance the event with external metadata. This section defines a list of\n# processors
    that are applied one by one and the first one receives the initial\n# event:\n#\n#
    \  event -> filter1 -> event1 -> filter2 ->event2 ...\n#\n# The supported processors
    are drop_fields, drop_event, include_fields, and\n# add_cloud_metadata.\n#\n#
    For example, you can use the following processors to keep the fields that\n# contain
    CPU load percentages, but remove the fields that contain CPU ticks\n# values:\n#\n#processors:\n#-
    include_fields:\n#    fields: [\"cpu\"]\n#- drop_fields:\n#    fields: [\"cpu.user\",
    \"cpu.system\"]\n#\n# The following example drops the events that have the HTTP
    response code 200:\n#\n#processors:\n#- drop_event:\n#    when:\n#       equals:\n#
    \          http.code: 200\n#\n# The following example enriches each event with
    metadata from the cloud\n# provider about the host machine. It works on EC2, GCE,
    DigitalOcean,\n# Tencent Cloud, and Alibaba Cloud.\n#\n#processors:\n#- add_cloud_metadata:
    ~\n#\n# The following example enriches each event with the machine's local time
    zone\n# offset from UTC.\n#\n#processors:\n#- add_locale:\n#    format: offset\n#\n#
    The following example enriches each event with docker metadata, it matches\n#
    given fields to an existing container id and adds info from that container:\n#\n#processors:\n#-
    add_docker_metadata:\n#    match_fields: [\"system.process.cgroup.id\"]\n#  host:
    \"unix:///var/run/docker.sock\"\n#  # To connect to Docker over TLS you must specify
    a client and CA certificate.\n#  #ssl:\n#  #  certificate_authority: \"/etc/pki/root/ca.pem\"\n#
    \ #  certificate:           \"/etc/pki/client/cert.pem\"\n#  #  key:                   \"/etc/pki/client/cert.key\"\n#\n\n#================================
    Outputs ======================================\n\n# Configure what outputs to
    use when sending the data collected by the beat.\n# Multiple outputs may be used.\n\n#--------------------------
    Elasticsearch output -------------------------------\n#output.elasticsearch:\n
    \ # Boolean flag to enable or disable the output module.\n  #enabled: true\n\n
    \ # Array of hosts to connect to.\n  # Scheme and port can be left out and will
    be set to the default (http and 9200)\n  # In case you specify and additional
    path, the scheme is required: http://localhost:9200/path\n  # IPv6 addresses should
    always be defined as: https://[2001:db8::1]:9200\n  #hosts: [\"localhost:9200\"]\n\n
    \ # Set gzip compression level.\n  #compression_level: 0\n\n  # Optional protocol
    and basic auth credentials.\n  #protocol: \"https\"\n  #username: \"elastic\"\n
    \ #password: \"changeme\"\n\n  # Dictionary of HTTP parameters to pass within
    the url with index operations.\n  #parameters:\n    #param1: value1\n    #param2:
    value2\n\n  # Number of workers per Elasticsearch host.\n  #worker: 1\n\n  # Optional
    index name. The default is \"filebeat\" plus date\n  # and generates [filebeat-]YYYY.MM.DD
    keys.\n  #index: \"filebeat-%{[beat.version]}-%{+yyyy.MM.dd}\"\n\n  # Optional
    ingest node pipeline. By default no pipeline will be used.\n  #pipeline: \"\"\n\n
    \ # Optional HTTP Path\n  #path: \"/elasticsearch\"\n\n  # Custom HTTP headers
    to add to each request\n  #headers:\n  #  X-My-Header: Contents of the header\n\n
    \ # Proxy server url\n  #proxy_url: http://proxy:3128\n\n  # The number of times
    a particular Elasticsearch index operation is attempted. If\n  # the indexing
    operation doesn't succeed after this many retries, the events are\n  # dropped.
    The default is 3.\n  #max_retries: 3\n\n  # The maximum number of events to bulk
    in a single Elasticsearch bulk API index request.\n  # The default is 50.\n  #bulk_max_size:
    50\n\n  # Configure http request timeout before failing an request to Elasticsearch.\n
    \ #timeout: 90\n\n  # The number of seconds to wait for new events between two
    bulk API index requests.\n  # If `bulk_max_size` is reached before this interval
    expires, addition bulk index\n  # requests are made.\n  #flush_interval: 1s\n\n
    \ # Use SSL settings for HTTPS. Default is true.\n  #ssl.enabled: true\n\n  #
    Configure SSL verification mode. If `none` is configured, all server hosts\n  #
    and certificates will be accepted. In this mode, SSL based connections are\n  #
    susceptible to man-in-the-middle attacks. Use only for testing. Default is\n  #
    `full`.\n  #ssl.verification_mode: full\n\n  # List of supported/valid TLS versions.
    By default all TLS versions 1.0 up to\n  # 1.2 are enabled.\n  #ssl.supported_protocols:
    [TLSv1.0, TLSv1.1, TLSv1.2]\n\n  # SSL configuration. By default is off.\n  #
    List of root certificates for HTTPS server verifications\n  #ssl.certificate_authorities:
    [\"/etc/pki/root/ca.pem\"]\n\n  # Certificate for SSL client authentication\n
    \ #ssl.certificate: \"/etc/pki/client/cert.pem\"\n\n  # Client Certificate Key\n
    \ #ssl.key: \"/etc/pki/client/cert.key\"\n\n  # Optional passphrase for decrypting
    the Certificate Key.\n  #ssl.key_passphrase: ''\n\n  # Configure cipher suites
    to be used for SSL connections\n  #ssl.cipher_suites: []\n\n  # Configure curve
    types for ECDHE based cipher suites\n  #ssl.curve_types: []\n\n\n#-----------------------------
    Logstash output ---------------------------------\n#output.logstash:\n  # Boolean
    flag to enable or disable the output module.\n  #enabled: true\n\n  # The Logstash
    hosts\n  #hosts: [\"localhost:5044\"]\n\n  # Number of workers per Logstash host.\n
    \ #worker: 1\n\n  # Set gzip compression level.\n  #compression_level: 3\n\n  #
    Optional load balance the events between the Logstash hosts\n  #loadbalance: true\n\n
    \ # Number of batches to be send asynchronously to logstash while processing\n
    \ # new batches.\n  #pipelining: 0\n\n  # Optional index name. The default index
    name is set to name of the beat\n  # in all lowercase.\n  #index: 'filebeat'\n\n
    \ # SOCKS5 proxy server URL\n  #proxy_url: socks5://user:password@socks5-server:2233\n\n
    \ # Resolve names locally when using a proxy server. Defaults to false.\n  #proxy_use_local_resolver:
    false\n\n  # Enable SSL support. SSL is automatically enabled, if any SSL setting
    is set.\n  #ssl.enabled: true\n\n  # Configure SSL verification mode. If `none`
    is configured, all server hosts\n  # and certificates will be accepted. In this
    mode, SSL based connections are\n  # susceptible to man-in-the-middle attacks.
    Use only for testing. Default is\n  # `full`.\n  #ssl.verification_mode: full\n\n
    \ # List of supported/valid TLS versions. By default all TLS versions 1.0 up to\n
    \ # 1.2 are enabled.\n  #ssl.supported_protocols: [TLSv1.0, TLSv1.1, TLSv1.2]\n\n
    \ # Optional SSL configuration options. SSL is off by default.\n  # List of root
    certificates for HTTPS server verifications\n  #ssl.certificate_authorities: [\"/etc/pki/root/ca.pem\"]\n\n
    \ # Certificate for SSL client authentication\n  #ssl.certificate: \"/etc/pki/client/cert.pem\"\n\n
    \ # Client Certificate Key\n  #ssl.key: \"/etc/pki/client/cert.key\"\n\n  # Optional
    passphrase for decrypting the Certificate Key.\n  #ssl.key_passphrase: ''\n\n
    \ # Configure cipher suites to be used for SSL connections\n  #ssl.cipher_suites:
    []\n\n  # Configure curve types for ECDHE based cipher suites\n  #ssl.curve_types:
    []\n\n#------------------------------- Kafka output ----------------------------------\n#output.kafka:\n
    \ # Boolean flag to enable or disable the output module.\n  #enabled: true\n\n
    \ # The list of Kafka broker addresses from where to fetch the cluster metadata.\n
    \ # The cluster metadata contain the actual Kafka brokers events are published\n
    \ # to.\n  #hosts: [\"localhost:9092\"]\n\n  # The Kafka topic used for produced
    events. The setting can be a format string\n  # using any event field. To set
    the topic from document type use `%{[type]}`.\n  #topic: beats\n\n  # The Kafka
    event key setting. Use format string to create unique event key.\n  # By default
    no event key will be generated.\n  #key: ''\n\n  # The Kafka event partitioning
    strategy. Default hashing strategy is `hash`\n  # using the `output.kafka.key`
    setting or randomly distributes events if\n  # `output.kafka.key` is not configured.\n
    \ #partition.hash:\n    # If enabled, events will only be published to partitions
    with reachable\n    # leaders. Default is false.\n    #reachable_only: false\n\n
    \   # Configure alternative event field names used to compute the hash value.\n
    \   # If empty `output.kafka.key` setting will be used.\n    # Default value is
    empty list.\n    #hash: []\n\n  # Authentication details. Password is required
    if username is set.\n  #username: ''\n  #password: ''\n\n  # Kafka version filebeat
    is assumed to run against. Defaults to the oldest\n  # supported stable version
    (currently version 0.8.2.0)\n  #version: 0.8.2\n\n  # Metadata update configuration.
    Metadata do contain leader information\n  # deciding which broker to use when
    publishing.\n  #metadata:\n    # Max metadata request retry attempts when cluster
    is in middle of leader\n    # election. Defaults to 3 retries.\n    #retry.max:
    3\n\n    # Waiting time between retries during leader elections. Default is 250ms.\n
    \   #retry.backoff: 250ms\n\n    # Refresh metadata interval. Defaults to every
    10 minutes.\n    #refresh_frequency: 10m\n\n  # The number of concurrent load-balanced
    Kafka output workers.\n  #worker: 1\n\n  # The number of times to retry publishing
    an event after a publishing failure.\n  # After the specified number of retries,
    the events are typically dropped.\n  # Some Beats, such as Filebeat, ignore the
    max_retries setting and retry until\n  # all events are published.  Set max_retries
    to a value less than 0 to retry\n  # until all events are published. The default
    is 3.\n  #max_retries: 3\n\n  # The maximum number of events to bulk in a single
    Kafka request. The default\n  # is 2048.\n  #bulk_max_size: 2048\n\n  # The number
    of seconds to wait for responses from the Kafka brokers before\n  # timing out.
    The default is 30s.\n  #timeout: 30s\n\n  # The maximum duration a broker will
    wait for number of required ACKs. The\n  # default is 10s.\n  #broker_timeout:
    10s\n\n  # The number of messages buffered for each Kafka broker. The default
    is 256.\n  #channel_buffer_size: 256\n\n  # The keep-alive period for an active
    network connection. If 0s, keep-alives\n  # are disabled. The default is 0 seconds.\n
    \ #keep_alive: 0\n\n  # Sets the output compression codec. Must be one of none,
    snappy and gzip. The\n  # default is gzip.\n  #compression: gzip\n\n  # The maximum
    permitted size of JSON-encoded messages. Bigger messages will be\n  # dropped.
    The default value is 1000000 (bytes). This value should be equal to\n  # or less
    than the broker's message.max.bytes.\n  #max_message_bytes: 1000000\n\n  # The
    ACK reliability level required from broker. 0=no response, 1=wait for\n  # local
    commit, -1=wait for all replicas to commit. The default is 1.  Note:\n  # If set
    to 0, no ACKs are returned by Kafka. Messages might be lost silently\n  # on error.\n
    \ #required_acks: 1\n\n  # The number of seconds to wait for new events between
    two producer API calls.\n  #flush_interval: 1s\n\n  # The configurable ClientID
    used for logging, debugging, and auditing\n  # purposes.  The default is \"beats\".\n
    \ #client_id: beats\n\n  # Enable SSL support. SSL is automatically enabled, if
    any SSL setting is set.\n  #ssl.enabled: true\n\n  # Optional SSL configuration
    options. SSL is off by default.\n  # List of root certificates for HTTPS server
    verifications\n  #ssl.certificate_authorities: [\"/etc/pki/root/ca.pem\"]\n\n
    \ # Configure SSL verification mode. If `none` is configured, all server hosts\n
    \ # and certificates will be accepted. In this mode, SSL based connections are\n
    \ # susceptible to man-in-the-middle attacks. Use only for testing. Default is\n
    \ # `full`.\n  #ssl.verification_mode: full\n\n  # List of supported/valid TLS
    versions. By default all TLS versions 1.0 up to\n  # 1.2 are enabled.\n  #ssl.supported_protocols:
    [TLSv1.0, TLSv1.1, TLSv1.2]\n\n  # Certificate for SSL client authentication\n
    \ #ssl.certificate: \"/etc/pki/client/cert.pem\"\n\n  # Client Certificate Key\n
    \ #ssl.key: \"/etc/pki/client/cert.key\"\n\n  # Optional passphrase for decrypting
    the Certificate Key.\n  #ssl.key_passphrase: ''\n\n  # Configure cipher suites
    to be used for SSL connections\n  #ssl.cipher_suites: []\n\n  # Configure curve
    types for ECDHE based cipher suites\n  #ssl.curve_types: []\n\n#-------------------------------
    Redis output ----------------------------------\noutput.redis:\n  # Boolean flag
    to enable or disable the output module.\n  enabled: true\n\n  # The list of Redis
    servers to connect to. If load balancing is enabled, the\n  # events are distributed
    to the servers in the list. If one server becomes\n  # unreachable, the events
    are distributed to the reachable servers only.\n  hosts: [\"localhost\"]\n\n  #
    The Redis port to use if hosts does not contain a port number. The default\n  #
    is 6379.\n  port: 6379\n\n  # The name of the Redis list or channel the events
    are published to. The\n  # default is filebeat.\n  key: filebeat\n\n  # The password
    to authenticate with. The default is no authentication.\n  #password:\n\n  # The
    Redis database number where the events are published. The default is 0.\n  db:
    0\n\n  # The Redis data type to use for publishing events. If the data type is
    list,\n  # the Redis RPUSH command is used. If the data type is channel, the Redis\n
    \ # PUBLISH command is used. The default value is list.\n  datatype: list\n\n
    \ # The number of workers to use for each host configured to publish events to\n
    \ # Redis. Use this setting along with the loadbalance option. For example, if\n
    \ # you have 2 hosts and 3 workers, in total 6 workers are started (3 for each\n
    \ # host).\n  #worker: 1\n\n  # If set to true and multiple hosts or workers are
    configured, the output\n  # plugin load balances published events onto all Redis
    hosts. If set to false,\n  # the output plugin sends all events to only one host
    (determined at random)\n  # and will switch to another host if the currently selected
    one becomes\n  # unreachable. The default value is true.\n  #loadbalance: true\n\n
    \ # The Redis connection timeout in seconds. The default is 5 seconds.\n # timeout:
    5s\n\n  # The number of times to retry publishing an event after a publishing
    failure.\n  # After the specified number of retries, the events are typically
    dropped.\n  # Some Beats, such as Filebeat, ignore the max_retries setting and
    retry until\n  # all events are published. Set max_retries to a value less than
    0 to retry\n  # until all events are published. The default is 3.\n  #max_retries:
    3\n\n  # The maximum number of events to bulk in a single Redis request or pipeline.\n
    \ # The default is 2048.\n  #bulk_max_size: 2048\n\n  # The URL of the SOCKS5
    proxy to use when connecting to the Redis servers. The\n  # value must be a URL
    with a scheme of socks5://.\n  #proxy_url:\n\n  # This option determines whether
    Redis hostnames are resolved locally when\n  # using a proxy. The default value
    is false, which means that name resolution\n  # occurs on the proxy server.\n
    \ #proxy_use_local_resolver: false\n\n  # Enable SSL support. SSL is automatically
    enabled, if any SSL setting is set.\n  #ssl.enabled: true\n\n  # Configure SSL
    verification mode. If `none` is configured, all server hosts\n  # and certificates
    will be accepted. In this mode, SSL based connections are\n  # susceptible to
    man-in-the-middle attacks. Use only for testing. Default is\n  # `full`.\n  #ssl.verification_mode:
    full\n\n  # List of supported/valid TLS versions. By default all TLS versions
    1.0 up to\n  # 1.2 are enabled.\n  #ssl.supported_protocols: [TLSv1.0, TLSv1.1,
    TLSv1.2]\n\n  # Optional SSL configuration options. SSL is off by default.\n  #
    List of root certificates for HTTPS server verifications\n  #ssl.certificate_authorities:
    [\"/etc/pki/root/ca.pem\"]\n\n  # Certificate for SSL client authentication\n
    \ #ssl.certificate: \"/etc/pki/client/cert.pem\"\n\n  # Client Certificate Key\n
    \ #ssl.key: \"/etc/pki/client/cert.key\"\n\n  # Optional passphrase for decrypting
    the Certificate Key.\n  #ssl.key_passphrase: ''\n\n  # Configure cipher suites
    to be used for SSL connections\n  #ssl.cipher_suites: []\n\n  # Configure curve
    types for ECDHE based cipher suites\n  #ssl.curve_types: []\n\n\n#-------------------------------
    File output -----------------------------------\n#output.file:\n  # Boolean flag
    to enable or disable the output module.\n  #enabled: true\n\n  # Path to the directory
    where to save the generated files. The option is\n  # mandatory.\n  #path: \"/tmp/filebeat\"\n\n
    \ # Name of the generated files. The default is `filebeat` and it generates\n
    \ # files: `filebeat`, `filebeat.1`, `filebeat.2`, etc.\n  #filename: filebeat\n\n
    \ # Maximum size in kilobytes of each file. When this size is reached, and on\n
    \ # every filebeat restart, the files are rotated. The default value is 10240\n
    \ # kB.\n  #rotate_every_kb: 10000\n\n  # Maximum number of files under path.
    When this number of files is reached,\n  # the oldest file is deleted and the
    rest are shifted from last to first. The\n  # default is 7 files.\n  #number_of_files:
    7\n\n\n#----------------------------- Console output ---------------------------------\n#output.console:\n
    \ # Boolean flag to enable or disable the output module.\n  #enabled: true\n\n
    \ # Pretty print json event\n  #pretty: false\n\n#=================================
    Paths ======================================\n\n# The home path for the filebeat
    installation. This is the default base path\n# for all other path settings and
    for miscellaneous files that come with the\n# distribution (for example, the sample
    dashboards).\n# If not set by a CLI flag or in the configuration file, the default
    for the\n# home path is the location of the binary.\n#path.home:\n\n# The configuration
    path for the filebeat installation. This is the default\n# base path for configuration
    files, including the main YAML configuration file\n# and the Elasticsearch template
    file. If not set by a CLI flag or in the\n# configuration file, the default for
    the configuration path is the home path.\n#path.config: ${path.home}\n\n# The
    data path for the filebeat installation. This is the default base path\n# for
    all the files in which filebeat needs to store its data. If not set by a\n# CLI
    flag or in the configuration file, the default for the data path is a data\n#
    subdirectory inside the home path.\n#path.data: ${path.home}/data\n\n# The logs
    path for a filebeat installation. This is the default location for\n# the Beat's
    log files. If not set by a CLI flag or in the configuration file,\n# the default
    for the logs path is a logs subdirectory inside the home path.\n#path.logs: ${path.home}/logs\n\n#==============================
    Dashboards =====================================\n# These settings control loading
    the sample dashboards to the Kibana index. Loading\n# the dashboards is disabled
    by default and can be enabled either by setting the\n# options here, or by using
    the `-setup` CLI flag.\n#setup.dashboards.enabled: false\n\n# The URL from where
    to download the dashboards archive. By default this URL\n# has a value which is
    computed based on the Beat name and version. For released\n# versions, this URL
    points to the dashboard archive on the artifacts.elastic.co\n# website.\n#setup.dashboards.url:\n\n#
    The directory from where to read the dashboards. It is used instead of the URL\n#
    when it has a value.\n#setup.dashboards.directory:\n\n# The file archive (zip
    file) from where to read the dashboards. It is used instead\n# of the URL when
    it has a value.\n#setup.dashboards.file:\n\n# If this option is enabled, the snapshot
    URL is used instead of the default URL.\n#setup.dashboards.snapshot: false\n\n#
    The URL from where to download the snapshot version of the dashboards. By default\n#
    this has a value which is computed based on the Beat name and version.\n#setup.dashboards.snapshot_url\n\n#
    In case the archive contains the dashboards from multiple Beats, this lets you\n#
    select which one to load. You can load all the dashboards in the archive by\n#
    setting this to the empty string.\n#setup.dashboards.beat: filebeat\n\n# The name
    of the Kibana index to use for setting the configuration. Default is \".kibana\"\n#setup.dashboards.kibana_index:
    .kibana\n\n# The Elasticsearch index name. This overwrites the index name defined
    in the\n# dashboards and index pattern. Example: testbeat-*\n#setup.dashboards.index:\n\n#==============================
    Template =====================================\n\n# A template is used to set
    the mapping in Elasticsearch\n# By default template loading is enabled and the
    template is loaded.\n# These settings can be adjusted to load your own template
    or overwrite existing ones.\n\n# Set to false to disable template loading.\n#setup.template.enabled:
    true\n\n# Template name. By default the template name is filebeat.\n# The version
    of the beat will always be appended to the given name\n# so the final name is
    filebeat-%{[beat.version]}.\n#setup.template.name: \"filebeat\"\n\n# Path to fields.yml
    file to generate the template\n#setup.template.fields: \"${path.config}/fields.yml\"\n\n#
    Overwrite existing template\n#setup.template.overwrite: false\n\n\n#================================
    HTTP Endpoint ======================================\n# Each beat can expose internal
    data points through a http endpoint. For security\n# reason the endpoint is disabled
    by default. This feature is currently in beta.\n\n# Defines if http endpoint is
    enabled\n#http.enabled: false\n\n# Host to expose the http endpoint to. It is
    recommended to use only localhost.\n#http.host: localhost\n\n# Port on which the
    http endpoint is exposed. Default is 5066\n#http.port: 5066\n\n#================================
    Logging ======================================\n# There are three options for
    the log output: syslog, file, stderr.\n# Under Windows systems, the log files
    are per default sent to the file output,\n# under all other system per default
    to syslog.\n\n# Sets log level. The default log level is info.\n# Available log
    levels are: critical, error, warning, info, debug\n#logging.level: info\n\n# Enable
    debug output for selected components. To enable all selectors use [\"*\"]\n# Other
    available selectors are \"beat\", \"publish\", \"service\"\n# Multiple selectors
    can be chained.\n#logging.selectors: [ ]\n\n# Send all logging output to syslog.
    The default is false.\n#logging.to_syslog: true\n\n# If enabled, filebeat periodically
    logs its internal metrics that have changed\n# in the last period. For each metric
    that changed, the delta from the value at\n# the beginning of the period is logged.
    Also, the total values for\n# all non-zero internal metrics are logged on shutdown.
    The default is true.\n#logging.metrics.enabled: true\n\n# The period after which
    to log the internal metrics. The default is 30s.\n#logging.metrics.period: 30s\n\n#
    Logging to rotating files files. Set logging.to_files to false to disable logging
    to\n# files.\n#logging.to_files: true\n#logging.files:\n  # Configure the path
    where the logs are written. The default is the logs directory\n  # under the home
    path (the binary location).\n  #path: /var/log/filebeat\n\n  # The name of the
    files where the logs are written to.\n  #name: filebeat\n\n  # Configure log file
    size limit. If limit is reached, log file will be\n  # automatically rotated\n
    \ #rotateeverybytes: 10485760 # = 10MB\n\n  # Number of rotated log files to keep.
    Oldest files will be deleted first.\n  #keepfiles: 7\n"
kind: ConfigMap
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"v1","data":{"filebeat.yml":"######################## Filebeat Configuration ############################\n\n# This file is a full configuration example documenting all non-deprecated\n# options in comments. For a shorter configuration example, that contains only\n# the most common options, please see filebeat.yml in the same directory.\n#\n# You can find the full configuration reference here:\n# https://www.elastic.co/guide/en/beats/filebeat/index.html\n\n\n#==========================  Modules configuration ============================\nfilebeat.modules:   \n\n\n#=========================== Filebeat prospectors =============================\n\n# List of prospectors to fetch data.\nfilebeat.prospectors:\n# Each - is a prospector. Most options can be set at the prospector level, so\n# you can use different prospectors for various configurations.\n# Below are the prospector specific configurations.\n\n# Type of the files. Based on this the way the file is read is decided.\n# The different types cannot be mixed in one prospector\n\n#------------------------------ Log prospector --------------------------------\n- type: log\n  paths:\n    - /bro/logger/*.log\n  tags: [\"bro\"]\n\n  ### JSON configuration\n\n  # Decode JSON options. Enable this if your logs are structured in JSON.\n  # JSON key on which to apply the line filtering and multiline settings. This key\n  # must be top level and its value must be string, otherwise it is ignored. If\n  # no text key is defined, the line filtering and multiline features cannot be used.\n  #json.message_key:\n\n  # By default, the decoded JSON is placed under a \"json\" key in the output document.\n  # If you enable this setting, the keys are copied top level in the output document.\n  #json.keys_under_root: false\n\n  # If keys_under_root and this setting are enabled, then the values from the decoded\n  # JSON object overwrite the fields that Filebeat normally adds (type, source, offset, etc.)\n  # in case of conflicts.\n  #json.overwrite_keys: false\n\n  # If this setting is enabled, Filebeat adds a \"error.message\" and \"error.key: json\" key in case of JSON\n  # unmarshaling errors or when a text key is defined in the configuration but cannot\n  # be used.\n  #json.add_error_key: true\n\n  ### Multiline options\n\n  # Mutiline can be used for log messages spanning multiple lines. This is common\n  # for Java Stack Traces or C-Line Continuation\n\n  # The regexp Pattern that has to be matched. The example pattern matches all lines starting with [\n  #multiline.pattern: ^\\[\n\n  # Defines if the pattern set under pattern should be negated or not. Default is false.\n  #multiline.negate: false\n\n  # Match can be set to \"after\" or \"before\". It is used to define if lines should be append to a pattern\n  # that was (not) matched before or after or as long as a pattern is not matched based on negate.\n  # Note: After is the equivalent to previous and before is the equivalent to to next in Logstash\n  #multiline.match: after\n\n  # The maximum number of lines that are combined to one event.\n  # In case there are more the max_lines the additional lines are discarded.\n  # Default is 500\n  #multiline.max_lines: 500\n\n  # After the defined timeout, an multiline event is sent even if no new pattern was found to start a new event\n  # Default is 5s.\n  #multiline.timeout: 5s\n\n  # Setting tail_files to true means filebeat starts reading new files at the end\n  # instead of the beginning. If this is used in combination with log rotation\n  # this can mean that the first entries of a new file are skipped.\n  #tail_files: false\n\n  # The Ingest Node pipeline ID associated with this prospector. If this is set, it\n  # overwrites the pipeline option from the Elasticsearch output.\n  #pipeline:\n\n  # If symlinks is enabled, symlinks are opened and harvested. The harvester is openening the\n  # original for harvesting but will report the symlink name as source.\n  #symlinks: false\n\n  # Backoff values define how aggressively filebeat crawls new files for updates\n  # The default values can be used in most cases. Backoff defines how long it is waited\n  # to check a file again after EOF is reached. Default is 1s which means the file\n  # is checked every second if new lines were added. This leads to a near real time crawling.\n  # Every time a new line appears, backoff is reset to the initial value.\n  #backoff: 1s\n\n  # Max backoff defines what the maximum backoff time is. After having backed off multiple times\n  # from checking the files, the waiting time will never exceed max_backoff independent of the\n  # backoff factor. Having it set to 10s means in the worst case a new line can be added to a log\n  # file after having backed off multiple times, it takes a maximum of 10s to read the new line\n  #max_backoff: 10s\n\n  # The backoff factor defines how fast the algorithm backs off. The bigger the backoff factor,\n  # the faster the max_backoff value is reached. If this value is set to 1, no backoff will happen.\n  # The backoff value will be multiplied each time with the backoff_factor until max_backoff is reached\n  #backoff_factor: 2\n\n  # Max number of harvesters that are started in parallel.\n  # Default is 0 which means unlimited\n  #harvester_limit: 0\n\n  ### Harvester closing options\n\n  # Close inactive closes the file handler after the predefined period.\n  # The period starts when the last line of the file was, not the file ModTime.\n  # Time strings like 2h (2 hours), 5m (5 minutes) can be used.\n  #close_inactive: 5m\n\n  # Close renamed closes a file handler when the file is renamed or rotated.\n  # Note: Potential data loss. Make sure to read and understand the docs for this option.\n  #close_renamed: false\n\n  # When enabling this option, a file handler is closed immediately in case a file can't be found\n  # any more. In case the file shows up again later, harvesting will continue at the last known position\n  # after scan_frequency.\n  #close_removed: true\n\n  # Closes the file handler as soon as the harvesters reaches the end of the file.\n  # By default this option is disabled.\n  # Note: Potential data loss. Make sure to read and understand the docs for this option.\n  #close_eof: false\n\n  ### State options\n\n  # Files for the modification data is older then clean_inactive the state from the registry is removed\n  # By default this is disabled.\n  #clean_inactive: \n  # Removes the state for file which cannot be found on disk anymore immediately\n  #clean_removed: true\n\n  # Close timeout closes the harvester after the predefined time.\n  # This is independent if the harvester did finish reading the file or not.\n  # By default this option is disabled.\n  # Note: Potential data loss. Make sure to read and understand the docs for this option.\n  #close_timeout: 0\n\n  # Defines if prospectors is enabled\n  #enabled: true\n\n#----------------------------- Stdin prospector -------------------------------\n# Configuration to use stdin input\n#- type: stdin\n\n#------------------------- Redis slowlog prospector ---------------------------\n# Experimental: Config options for the redis slow log prospector\n#- type: redis\n #hosts: [\"localhost:6379\"]\n  #username:\n  #password:\n  #enabled: true\n  #scan_frequency: 10s\n\n  # Timeout after which time the prospector should return an error\n  #timeout: 1s\n\n  # Network type to be used for redis connection. Default: tcp\n  #network: tcp\n\n  # Max number of concurrent connections. Default: 10\n  #maxconn: 10\n\n  # Redis AUTH password. Empty by default.\n  #password: foobared\n\n#========================= Filebeat global options ============================\n\n# Event count spool threshold - forces network flush if exceeded\n#filebeat.spool_size: 2048\n\n# Enable async publisher pipeline in filebeat (Experimental!)\n#filebeat.publish_async: false\n\n# Defines how often the spooler is flushed. After idle_timeout the spooler is\n# Flush even though spool_size is not reached.\n#filebeat.idle_timeout: 5s\n\n# Name of the registry file. If a relative path is used, it is considered relative to the\n# data path.\n#filebeat.registry_file: ${path.data}/registry\n\n#\n# These config files must have the full filebeat config part inside, but only\n# the prospector part is processed. All global options like spool_size are ignored.\n# The config_dir MUST point to a different directory then where the main filebeat config file is in.\n#filebeat.config_dir:\n\n# How long filebeat waits on shutdown for the publisher to finish.\n# Default is 0, not waiting.\n#filebeat.shutdown_timeout: 0\n\n# Enable filebeat config reloading\n#filebeat.config.prospectors:\n  #enabled: false\n  #path: configs/*.yml\n  #reload.enabled: true\n  #reload.period: 10s\n\n#================================ General ======================================\n\n# The name of the shipper that publishes the network data. It can be used to group\n# all the transactions sent by a single shipper in the web interface.\n# If this options is not defined, the hostname is used.\n#name:\n\n# The tags of the shipper are included in their own field with each\n# transaction published. Tags make it easy to group servers by different\n# logical properties.\n#tags: [\"service-X\", \"web-tier\"]\n\n# Optional fields that you can specify to add additional information to the\n# output. Fields can be scalar values, arrays, dictionaries, or any nested\n# combination of these.\n#fields:\n#  env: staging\n\n# If this option is set to true, the custom fields are stored as top-level\n# fields in the output document instead of being grouped under a fields\n# sub-dictionary. Default is false.\n#fields_under_root: false\n\n# Internal queue size for single events in processing pipeline\n#queue_size: 1000\n\n# The internal queue size for bulk events in the processing pipeline.\n# Do not modify this value.\n#bulk_queue_size: 0\n\n# Sets the maximum number of CPUs that can be executing simultaneously. The\n# default is the number of logical CPUs available in the system.\n#max_procs:\n\n#================================ Processors ===================================\n\n# Processors are used to reduce the number of fields in the exported event or to\n# enhance the event with external metadata. This section defines a list of\n# processors that are applied one by one and the first one receives the initial\n# event:\n#\n#   event -\u003e filter1 -\u003e event1 -\u003e filter2 -\u003eevent2 ...\n#\n# The supported processors are drop_fields, drop_event, include_fields, and\n# add_cloud_metadata.\n#\n# For example, you can use the following processors to keep the fields that\n# contain CPU load percentages, but remove the fields that contain CPU ticks\n# values:\n#\n#processors:\n#- include_fields:\n#    fields: [\"cpu\"]\n#- drop_fields:\n#    fields: [\"cpu.user\", \"cpu.system\"]\n#\n# The following example drops the events that have the HTTP response code 200:\n#\n#processors:\n#- drop_event:\n#    when:\n#       equals:\n#           http.code: 200\n#\n# The following example enriches each event with metadata from the cloud\n# provider about the host machine. It works on EC2, GCE, DigitalOcean,\n# Tencent Cloud, and Alibaba Cloud.\n#\n#processors:\n#- add_cloud_metadata: ~\n#\n# The following example enriches each event with the machine's local time zone\n# offset from UTC.\n#\n#processors:\n#- add_locale:\n#    format: offset\n#\n# The following example enriches each event with docker metadata, it matches\n# given fields to an existing container id and adds info from that container:\n#\n#processors:\n#- add_docker_metadata:\n#    match_fields: [\"system.process.cgroup.id\"]\n#  host: \"unix:///var/run/docker.sock\"\n#  # To connect to Docker over TLS you must specify a client and CA certificate.\n#  #ssl:\n#  #  certificate_authority: \"/etc/pki/root/ca.pem\"\n#  #  certificate:           \"/etc/pki/client/cert.pem\"\n#  #  key:                   \"/etc/pki/client/cert.key\"\n#\n\n#================================ Outputs ======================================\n\n# Configure what outputs to use when sending the data collected by the beat.\n# Multiple outputs may be used.\n\n#-------------------------- Elasticsearch output -------------------------------\n#output.elasticsearch:\n  # Boolean flag to enable or disable the output module.\n  #enabled: true\n\n  # Array of hosts to connect to.\n  # Scheme and port can be left out and will be set to the default (http and 9200)\n  # In case you specify and additional path, the scheme is required: http://localhost:9200/path\n  # IPv6 addresses should always be defined as: https://[2001:db8::1]:9200\n  #hosts: [\"localhost:9200\"]\n\n  # Set gzip compression level.\n  #compression_level: 0\n\n  # Optional protocol and basic auth credentials.\n  #protocol: \"https\"\n  #username: \"elastic\"\n  #password: \"changeme\"\n\n  # Dictionary of HTTP parameters to pass within the url with index operations.\n  #parameters:\n    #param1: value1\n    #param2: value2\n\n  # Number of workers per Elasticsearch host.\n  #worker: 1\n\n  # Optional index name. The default is \"filebeat\" plus date\n  # and generates [filebeat-]YYYY.MM.DD keys.\n  #index: \"filebeat-%{[beat.version]}-%{+yyyy.MM.dd}\"\n\n  # Optional ingest node pipeline. By default no pipeline will be used.\n  #pipeline: \"\"\n\n  # Optional HTTP Path\n  #path: \"/elasticsearch\"\n\n  # Custom HTTP headers to add to each request\n  #headers:\n  #  X-My-Header: Contents of the header\n\n  # Proxy server url\n  #proxy_url: http://proxy:3128\n\n  # The number of times a particular Elasticsearch index operation is attempted. If\n  # the indexing operation doesn't succeed after this many retries, the events are\n  # dropped. The default is 3.\n  #max_retries: 3\n\n  # The maximum number of events to bulk in a single Elasticsearch bulk API index request.\n  # The default is 50.\n  #bulk_max_size: 50\n\n  # Configure http request timeout before failing an request to Elasticsearch.\n  #timeout: 90\n\n  # The number of seconds to wait for new events between two bulk API index requests.\n  # If `bulk_max_size` is reached before this interval expires, addition bulk index\n  # requests are made.\n  #flush_interval: 1s\n\n  # Use SSL settings for HTTPS. Default is true.\n  #ssl.enabled: true\n\n  # Configure SSL verification mode. If `none` is configured, all server hosts\n  # and certificates will be accepted. In this mode, SSL based connections are\n  # susceptible to man-in-the-middle attacks. Use only for testing. Default is\n  # `full`.\n  #ssl.verification_mode: full\n\n  # List of supported/valid TLS versions. By default all TLS versions 1.0 up to\n  # 1.2 are enabled.\n  #ssl.supported_protocols: [TLSv1.0, TLSv1.1, TLSv1.2]\n\n  # SSL configuration. By default is off.\n  # List of root certificates for HTTPS server verifications\n  #ssl.certificate_authorities: [\"/etc/pki/root/ca.pem\"]\n\n  # Certificate for SSL client authentication\n  #ssl.certificate: \"/etc/pki/client/cert.pem\"\n\n  # Client Certificate Key\n  #ssl.key: \"/etc/pki/client/cert.key\"\n\n  # Optional passphrase for decrypting the Certificate Key.\n  #ssl.key_passphrase: ''\n\n  # Configure cipher suites to be used for SSL connections\n  #ssl.cipher_suites: []\n\n  # Configure curve types for ECDHE based cipher suites\n  #ssl.curve_types: []\n\n\n#----------------------------- Logstash output ---------------------------------\n#output.logstash:\n  # Boolean flag to enable or disable the output module.\n  #enabled: true\n\n  # The Logstash hosts\n  #hosts: [\"localhost:5044\"]\n\n  # Number of workers per Logstash host.\n  #worker: 1\n\n  # Set gzip compression level.\n  #compression_level: 3\n\n  # Optional load balance the events between the Logstash hosts\n  #loadbalance: true\n\n  # Number of batches to be send asynchronously to logstash while processing\n  # new batches.\n  #pipelining: 0\n\n  # Optional index name. The default index name is set to name of the beat\n  # in all lowercase.\n  #index: 'filebeat'\n\n  # SOCKS5 proxy server URL\n  #proxy_url: socks5://user:password@socks5-server:2233\n\n  # Resolve names locally when using a proxy server. Defaults to false.\n  #proxy_use_local_resolver: false\n\n  # Enable SSL support. SSL is automatically enabled, if any SSL setting is set.\n  #ssl.enabled: true\n\n  # Configure SSL verification mode. If `none` is configured, all server hosts\n  # and certificates will be accepted. In this mode, SSL based connections are\n  # susceptible to man-in-the-middle attacks. Use only for testing. Default is\n  # `full`.\n  #ssl.verification_mode: full\n\n  # List of supported/valid TLS versions. By default all TLS versions 1.0 up to\n  # 1.2 are enabled.\n  #ssl.supported_protocols: [TLSv1.0, TLSv1.1, TLSv1.2]\n\n  # Optional SSL configuration options. SSL is off by default.\n  # List of root certificates for HTTPS server verifications\n  #ssl.certificate_authorities: [\"/etc/pki/root/ca.pem\"]\n\n  # Certificate for SSL client authentication\n  #ssl.certificate: \"/etc/pki/client/cert.pem\"\n\n  # Client Certificate Key\n  #ssl.key: \"/etc/pki/client/cert.key\"\n\n  # Optional passphrase for decrypting the Certificate Key.\n  #ssl.key_passphrase: ''\n\n  # Configure cipher suites to be used for SSL connections\n  #ssl.cipher_suites: []\n\n  # Configure curve types for ECDHE based cipher suites\n  #ssl.curve_types: []\n\n#------------------------------- Kafka output ----------------------------------\n#output.kafka:\n  # Boolean flag to enable or disable the output module.\n  #enabled: true\n\n  # The list of Kafka broker addresses from where to fetch the cluster metadata.\n  # The cluster metadata contain the actual Kafka brokers events are published\n  # to.\n  #hosts: [\"localhost:9092\"]\n\n  # The Kafka topic used for produced events. The setting can be a format string\n  # using any event field. To set the topic from document type use `%{[type]}`.\n  #topic: beats\n\n  # The Kafka event key setting. Use format string to create unique event key.\n  # By default no event key will be generated.\n  #key: ''\n\n  # The Kafka event partitioning strategy. Default hashing strategy is `hash`\n  # using the `output.kafka.key` setting or randomly distributes events if\n  # `output.kafka.key` is not configured.\n  #partition.hash:\n    # If enabled, events will only be published to partitions with reachable\n    # leaders. Default is false.\n    #reachable_only: false\n\n    # Configure alternative event field names used to compute the hash value.\n    # If empty `output.kafka.key` setting will be used.\n    # Default value is empty list.\n    #hash: []\n\n  # Authentication details. Password is required if username is set.\n  #username: ''\n  #password: ''\n\n  # Kafka version filebeat is assumed to run against. Defaults to the oldest\n  # supported stable version (currently version 0.8.2.0)\n  #version: 0.8.2\n\n  # Metadata update configuration. Metadata do contain leader information\n  # deciding which broker to use when publishing.\n  #metadata:\n    # Max metadata request retry attempts when cluster is in middle of leader\n    # election. Defaults to 3 retries.\n    #retry.max: 3\n\n    # Waiting time between retries during leader elections. Default is 250ms.\n    #retry.backoff: 250ms\n\n    # Refresh metadata interval. Defaults to every 10 minutes.\n    #refresh_frequency: 10m\n\n  # The number of concurrent load-balanced Kafka output workers.\n  #worker: 1\n\n  # The number of times to retry publishing an event after a publishing failure.\n  # After the specified number of retries, the events are typically dropped.\n  # Some Beats, such as Filebeat, ignore the max_retries setting and retry until\n  # all events are published.  Set max_retries to a value less than 0 to retry\n  # until all events are published. The default is 3.\n  #max_retries: 3\n\n  # The maximum number of events to bulk in a single Kafka request. The default\n  # is 2048.\n  #bulk_max_size: 2048\n\n  # The number of seconds to wait for responses from the Kafka brokers before\n  # timing out. The default is 30s.\n  #timeout: 30s\n\n  # The maximum duration a broker will wait for number of required ACKs. The\n  # default is 10s.\n  #broker_timeout: 10s\n\n  # The number of messages buffered for each Kafka broker. The default is 256.\n  #channel_buffer_size: 256\n\n  # The keep-alive period for an active network connection. If 0s, keep-alives\n  # are disabled. The default is 0 seconds.\n  #keep_alive: 0\n\n  # Sets the output compression codec. Must be one of none, snappy and gzip. The\n  # default is gzip.\n  #compression: gzip\n\n  # The maximum permitted size of JSON-encoded messages. Bigger messages will be\n  # dropped. The default value is 1000000 (bytes). This value should be equal to\n  # or less than the broker's message.max.bytes.\n  #max_message_bytes: 1000000\n\n  # The ACK reliability level required from broker. 0=no response, 1=wait for\n  # local commit, -1=wait for all replicas to commit. The default is 1.  Note:\n  # If set to 0, no ACKs are returned by Kafka. Messages might be lost silently\n  # on error.\n  #required_acks: 1\n\n  # The number of seconds to wait for new events between two producer API calls.\n  #flush_interval: 1s\n\n  # The configurable ClientID used for logging, debugging, and auditing\n  # purposes.  The default is \"beats\".\n  #client_id: beats\n\n  # Enable SSL support. SSL is automatically enabled, if any SSL setting is set.\n  #ssl.enabled: true\n\n  # Optional SSL configuration options. SSL is off by default.\n  # List of root certificates for HTTPS server verifications\n  #ssl.certificate_authorities: [\"/etc/pki/root/ca.pem\"]\n\n  # Configure SSL verification mode. If `none` is configured, all server hosts\n  # and certificates will be accepted. In this mode, SSL based connections are\n  # susceptible to man-in-the-middle attacks. Use only for testing. Default is\n  # `full`.\n  #ssl.verification_mode: full\n\n  # List of supported/valid TLS versions. By default all TLS versions 1.0 up to\n  # 1.2 are enabled.\n  #ssl.supported_protocols: [TLSv1.0, TLSv1.1, TLSv1.2]\n\n  # Certificate for SSL client authentication\n  #ssl.certificate: \"/etc/pki/client/cert.pem\"\n\n  # Client Certificate Key\n  #ssl.key: \"/etc/pki/client/cert.key\"\n\n  # Optional passphrase for decrypting the Certificate Key.\n  #ssl.key_passphrase: ''\n\n  # Configure cipher suites to be used for SSL connections\n  #ssl.cipher_suites: []\n\n  # Configure curve types for ECDHE based cipher suites\n  #ssl.curve_types: []\n\n#------------------------------- Redis output ----------------------------------\noutput.redis:\n  # Boolean flag to enable or disable the output module.\n  enabled: true\n\n  # The list of Redis servers to connect to. If load balancing is enabled, the\n  # events are distributed to the servers in the list. If one server becomes\n  # unreachable, the events are distributed to the reachable servers only.\n  hosts: [\"localhost\"]\n\n  # The Redis port to use if hosts does not contain a port number. The default\n  # is 6379.\n  port: 6379\n\n  # The name of the Redis list or channel the events are published to. The\n  # default is filebeat.\n  key: filebeat\n\n  # The password to authenticate with. The default is no authentication.\n  #password:\n\n  # The Redis database number where the events are published. The default is 0.\n  db: 0\n\n  # The Redis data type to use for publishing events. If the data type is list,\n  # the Redis RPUSH command is used. If the data type is channel, the Redis\n  # PUBLISH command is used. The default value is list.\n  datatype: list\n\n  # The number of workers to use for each host configured to publish events to\n  # Redis. Use this setting along with the loadbalance option. For example, if\n  # you have 2 hosts and 3 workers, in total 6 workers are started (3 for each\n  # host).\n  #worker: 1\n\n  # If set to true and multiple hosts or workers are configured, the output\n  # plugin load balances published events onto all Redis hosts. If set to false,\n  # the output plugin sends all events to only one host (determined at random)\n  # and will switch to another host if the currently selected one becomes\n  # unreachable. The default value is true.\n  #loadbalance: true\n\n  # The Redis connection timeout in seconds. The default is 5 seconds.\n # timeout: 5s\n\n  # The number of times to retry publishing an event after a publishing failure.\n  # After the specified number of retries, the events are typically dropped.\n  # Some Beats, such as Filebeat, ignore the max_retries setting and retry until\n  # all events are published. Set max_retries to a value less than 0 to retry\n  # until all events are published. The default is 3.\n  #max_retries: 3\n\n  # The maximum number of events to bulk in a single Redis request or pipeline.\n  # The default is 2048.\n  #bulk_max_size: 2048\n\n  # The URL of the SOCKS5 proxy to use when connecting to the Redis servers. The\n  # value must be a URL with a scheme of socks5://.\n  #proxy_url:\n\n  # This option determines whether Redis hostnames are resolved locally when\n  # using a proxy. The default value is false, which means that name resolution\n  # occurs on the proxy server.\n  #proxy_use_local_resolver: false\n\n  # Enable SSL support. SSL is automatically enabled, if any SSL setting is set.\n  #ssl.enabled: true\n\n  # Configure SSL verification mode. If `none` is configured, all server hosts\n  # and certificates will be accepted. In this mode, SSL based connections are\n  # susceptible to man-in-the-middle attacks. Use only for testing. Default is\n  # `full`.\n  #ssl.verification_mode: full\n\n  # List of supported/valid TLS versions. By default all TLS versions 1.0 up to\n  # 1.2 are enabled.\n  #ssl.supported_protocols: [TLSv1.0, TLSv1.1, TLSv1.2]\n\n  # Optional SSL configuration options. SSL is off by default.\n  # List of root certificates for HTTPS server verifications\n  #ssl.certificate_authorities: [\"/etc/pki/root/ca.pem\"]\n\n  # Certificate for SSL client authentication\n  #ssl.certificate: \"/etc/pki/client/cert.pem\"\n\n  # Client Certificate Key\n  #ssl.key: \"/etc/pki/client/cert.key\"\n\n  # Optional passphrase for decrypting the Certificate Key.\n  #ssl.key_passphrase: ''\n\n  # Configure cipher suites to be used for SSL connections\n  #ssl.cipher_suites: []\n\n  # Configure curve types for ECDHE based cipher suites\n  #ssl.curve_types: []\n\n\n#------------------------------- File output -----------------------------------\n#output.file:\n  # Boolean flag to enable or disable the output module.\n  #enabled: true\n\n  # Path to the directory where to save the generated files. The option is\n  # mandatory.\n  #path: \"/tmp/filebeat\"\n\n  # Name of the generated files. The default is `filebeat` and it generates\n  # files: `filebeat`, `filebeat.1`, `filebeat.2`, etc.\n  #filename: filebeat\n\n  # Maximum size in kilobytes of each file. When this size is reached, and on\n  # every filebeat restart, the files are rotated. The default value is 10240\n  # kB.\n  #rotate_every_kb: 10000\n\n  # Maximum number of files under path. When this number of files is reached,\n  # the oldest file is deleted and the rest are shifted from last to first. The\n  # default is 7 files.\n  #number_of_files: 7\n\n\n#----------------------------- Console output ---------------------------------\n#output.console:\n  # Boolean flag to enable or disable the output module.\n  #enabled: true\n\n  # Pretty print json event\n  #pretty: false\n\n#================================= Paths ======================================\n\n# The home path for the filebeat installation. This is the default base path\n# for all other path settings and for miscellaneous files that come with the\n# distribution (for example, the sample dashboards).\n# If not set by a CLI flag or in the configuration file, the default for the\n# home path is the location of the binary.\n#path.home:\n\n# The configuration path for the filebeat installation. This is the default\n# base path for configuration files, including the main YAML configuration file\n# and the Elasticsearch template file. If not set by a CLI flag or in the\n# configuration file, the default for the configuration path is the home path.\n#path.config: ${path.home}\n\n# The data path for the filebeat installation. This is the default base path\n# for all the files in which filebeat needs to store its data. If not set by a\n# CLI flag or in the configuration file, the default for the data path is a data\n# subdirectory inside the home path.\n#path.data: ${path.home}/data\n\n# The logs path for a filebeat installation. This is the default location for\n# the Beat's log files. If not set by a CLI flag or in the configuration file,\n# the default for the logs path is a logs subdirectory inside the home path.\n#path.logs: ${path.home}/logs\n\n#============================== Dashboards =====================================\n# These settings control loading the sample dashboards to the Kibana index. Loading\n# the dashboards is disabled by default and can be enabled either by setting the\n# options here, or by using the `-setup` CLI flag.\n#setup.dashboards.enabled: false\n\n# The URL from where to download the dashboards archive. By default this URL\n# has a value which is computed based on the Beat name and version. For released\n# versions, this URL points to the dashboard archive on the artifacts.elastic.co\n# website.\n#setup.dashboards.url:\n\n# The directory from where to read the dashboards. It is used instead of the URL\n# when it has a value.\n#setup.dashboards.directory:\n\n# The file archive (zip file) from where to read the dashboards. It is used instead\n# of the URL when it has a value.\n#setup.dashboards.file:\n\n# If this option is enabled, the snapshot URL is used instead of the default URL.\n#setup.dashboards.snapshot: false\n\n# The URL from where to download the snapshot version of the dashboards. By default\n# this has a value which is computed based on the Beat name and version.\n#setup.dashboards.snapshot_url\n\n# In case the archive contains the dashboards from multiple Beats, this lets you\n# select which one to load. You can load all the dashboards in the archive by\n# setting this to the empty string.\n#setup.dashboards.beat: filebeat\n\n# The name of the Kibana index to use for setting the configuration. Default is \".kibana\"\n#setup.dashboards.kibana_index: .kibana\n\n# The Elasticsearch index name. This overwrites the index name defined in the\n# dashboards and index pattern. Example: testbeat-*\n#setup.dashboards.index:\n\n#============================== Template =====================================\n\n# A template is used to set the mapping in Elasticsearch\n# By default template loading is enabled and the template is loaded.\n# These settings can be adjusted to load your own template or overwrite existing ones.\n\n# Set to false to disable template loading.\n#setup.template.enabled: true\n\n# Template name. By default the template name is filebeat.\n# The version of the beat will always be appended to the given name\n# so the final name is filebeat-%{[beat.version]}.\n#setup.template.name: \"filebeat\"\n\n# Path to fields.yml file to generate the template\n#setup.template.fields: \"${path.config}/fields.yml\"\n\n# Overwrite existing template\n#setup.template.overwrite: false\n\n\n#================================ HTTP Endpoint ======================================\n# Each beat can expose internal data points through a http endpoint. For security\n# reason the endpoint is disabled by default. This feature is currently in beta.\n\n# Defines if http endpoint is enabled\n#http.enabled: false\n\n# Host to expose the http endpoint to. It is recommended to use only localhost.\n#http.host: localhost\n\n# Port on which the http endpoint is exposed. Default is 5066\n#http.port: 5066\n\n#================================ Logging ======================================\n# There are three options for the log output: syslog, file, stderr.\n# Under Windows systems, the log files are per default sent to the file output,\n# under all other system per default to syslog.\n\n# Sets log level. The default log level is info.\n# Available log levels are: critical, error, warning, info, debug\n#logging.level: info\n\n# Enable debug output for selected components. To enable all selectors use [\"*\"]\n# Other available selectors are \"beat\", \"publish\", \"service\"\n# Multiple selectors can be chained.\n#logging.selectors: [ ]\n\n# Send all logging output to syslog. The default is false.\n#logging.to_syslog: true\n\n# If enabled, filebeat periodically logs its internal metrics that have changed\n# in the last period. For each metric that changed, the delta from the value at\n# the beginning of the period is logged. Also, the total values for\n# all non-zero internal metrics are logged on shutdown. The default is true.\n#logging.metrics.enabled: true\n\n# The period after which to log the internal metrics. The default is 30s.\n#logging.metrics.period: 30s\n\n# Logging to rotating files files. Set logging.to_files to false to disable logging to\n# files.\n#logging.to_files: true\n#logging.files:\n  # Configure the path where the logs are written. The default is the logs directory\n  # under the home path (the binary location).\n  #path: /var/log/filebeat\n\n  # The name of the files where the logs are written to.\n  #name: filebeat\n\n  # Configure log file size limit. If limit is reached, log file will be\n  # automatically rotated\n  #rotateeverybytes: 10485760 # = 10MB\n\n  # Number of rotated log files to keep. Oldest files will be deleted first.\n  #keepfiles: 7\n"},"kind":"ConfigMap","metadata":{"annotations":{},"creationTimestamp":null,"name":"filebeat-bro-config","namespace":"default","selfLink":"/api/v1/namespaces/default/configmaps/filebeat-bro-config"}}
  creationTimestamp: null
  name: filebeat-bro-config
  selfLink: /api/v1/namespaces/default/configmaps/filebeat-bro-config
